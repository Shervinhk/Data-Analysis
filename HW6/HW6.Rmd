---
title: "HW6"
author: "Shervin Hakimi"
date: "4/12/2018"
output:

   prettydoc::html_pretty:
    theme: cayman
    highlight: github
---


## Question 1:
In this question we will use the `house` data.  
With this data we will find the correlation between the variables with `cor` function.  
We also considered the variables which will be considered as factors but their correlation value is pretty low.  
After that we will plot the correlations.  
By using the `rcorr` function from `Hmisc` library we will find the p-values.  
In the end we will sort the top ten highest values of correlation.  


```{r message=FALSE}
#HW6: Q1
#-----------------------------------------
# Libraries:
library(polycor)
library(corrplot)
library(Hmisc)
library(dplyr)
library(ggplot2)
library(gridExtra)

#-----------------------------------------
#Reading the data:
house = read.csv("/Users/shervin/Downloads/house/train.csv")


#-----------------------------------------
# Values w/o factors:
house_no_factors = house[,sapply(house, is.numeric)]
house_no_factors$Id <- NULL
house_no_factors[is.na(house_no_factors)] <- 0


#-----------------------------------------
#Value of Correlation:
res <- cor(house_no_factors)

#-----------------------------------------
#Plotting the correlations:
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

#-----------------------------------------
#Value of P-Values:
res2 <- rcorr(as.matrix(house_no_factors))


#-----------------------------------------
# P-Values:
house_no_factors = res2$P
sorted =sort(house_no_factors[,"SalePrice"], decreasing = FALSE)
sorted = as.data.frame(sorted)
c = rownames(sorted)
sorted %>% 
  mutate(name = c) -> sorted

ss_pvalue <- tableGrob(sorted)
plot(ss_pvalue[1:20,])

#-----------------------------------------
#Correlation:
house_price = res[,"SalePrice"]
sorted =sort(house_price, decreasing = TRUE)
sorted = as.data.frame(sorted)

c = rownames(sorted)
sorted %>% 
  mutate(name = c) -> sorted

cc = ggplot(data = sorted[1:11,], aes(x = name , y = sorted)) +
  geom_point() +
  theme_bw() 
#-----------------------------------------
#ploting the 10 highest correlated variables: w/o factors
ss <- tableGrob(sorted[1:11,])
plot(ss)




#-----------------------------------------
# Values with factors:

house_value = as.data.frame(lapply(house,as.numeric))
house_value$Id <- NULL
house_value =house_value[colSums(!is.na(house_value)) > 0]
house_value[is.na(house_value)] <- 0

#-----------------------------------------
#Value of Correlation:
res <- cor(house_value)

#-----------------------------------------
#Plotting the correlations:
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

#-----------------------------------------
#Value of P-Values:
res2 <- rcorr(as.matrix(house_value))



#-----------------------------------------
# P-Values:
house_p_value = res2$P
sorted =sort(house_p_value[,"SalePrice"], decreasing = FALSE)
sorted = as.data.frame(sorted)
c = rownames(sorted)
sorted %>% 
  mutate(name = c) -> sorted

ss_pvalue <- tableGrob(sorted)
plot(ss_pvalue[1:20,])

#-----------------------------------------
#Correlation:
house_price = res[,"SalePrice"]
sorted =sort(house_price, decreasing = TRUE)
sorted = as.data.frame(sorted)

c = rownames(sorted)
sorted %>% 
  mutate(name = c) -> sorted

cc = ggplot(data = sorted[1:11,], aes(x = name , y = sorted)) +
  geom_point() +
  theme_bw() 
#-----------------------------------------
#ploting the 10 highest correlated variables:
ss <- tableGrob(sorted[1:11,])
plot(ss)





#-----------------------------------------

```
We can see there is no difference between these two.  
This due to the fact that we don't know how different every step of factor is from one another.  
We can't simply assume it to be just 1 in every step.  

## Question 2:
We will define a `function` and using this `function` we will plot the desired graphs side by side.  

```{r message= FALSE, warning=FALSE }
#HW6: 2
#-----------------------------------------
# Libraries:
library(ggplot2)
library(gridExtra)
library(GGally)

#-----------------------------------------
# Reading our data and assuming levels for our factors:
house = read.csv("/Users/shervin/Downloads/house/train.csv")
house_value = as.data.frame(lapply(house,as.numeric))

#-----------------------------------------
#removing id:
house_value$Id <- NULL

#-----------------------------------------
# defining a function to plot all the plots togethers in one plot:

will_plot <- function(data, mapping, ...){
  p <- ggplot(data = data, mapping  = mapping) + 
    geom_point() + 
    geom_smooth(method=loess, fill="green", color="green", ...) +
    geom_smooth(method=lm, fill="red", color="red", ...)
  p
}

#-----------------------------------------
# ploting (with/o factors):

house_no_factors = house[,sapply(house, is.numeric)]
house_no_factors$Id <- NULL
house_no_factors[is.na(house_no_factors)] <- 0

z2 = ncol(house_no_factors)
ggpairs(house_no_factors[,1:10], lower = list(continuous = will_plot))

# we will plot the first 10, because it is huge


#-----------------------------------------
# plotting all will give us:
ggpairs(house_no_factors[,1:20], lower = list(continuous = will_plot))




```
We only plotted some of the plots for the first plot because plotting all the graphs will make a huge plot which is not really usable.  
The second plot is for most of the plots and its huge.

## Question 3:
Using the top ten correlated variables respect to `SalePrice` , we will make a linear model using `lm`.  


```{r message = FALSE}
# HW6 -3: 
# We will use the 10 most correlated variables:
library(ggplot2)
library(gridExtra)
library(GGally)

#-----------------------------------------
# from question 1 we have the 10 most correlated variables 
# to SalePrice
house = read.csv("/Users/shervin/Downloads/house/train.csv")
fit = lm(SalePrice ~ OverallQual +
                     GrLivArea +
                     GarageCars +
                     GarageArea +
                     TotalBsmtSF +
                     X1stFlrSF +
                     FullBath +
                     TotRmsAbvGrd +
                     YearBuilt +
                     YearRemodAdd, data = house)
summary(fit)

#-----------------------------------------



```


# Question 4 and 5:
The linear model gives us the coeficients and using this model we can predict.  
So using the same data as train data we will have: 
We will plot the predicted value of sale versus the real value.  
The line indicates `y=x` which is our desired line.
```{r message=FALSE}
# HW6 -4: 
# We will use the 10 most correlated variables:
library(ggplot2)
library(gridExtra)
library(GGally)

#-----------------------------------------
# from question 1 we have the 10 most correlated variables 
# to SalePrice
house = read.csv("/Users/shervin/Downloads/house/train.csv")
fit <- lm(SalePrice ~ OverallQual +
           GrLivArea +
           GarageCars +
           GarageArea +
           TotalBsmtSF +
           X1stFlrSF +
           FullBath +
           TotRmsAbvGrd +
           YearBuilt +
           YearRemodAdd, data = house)
summary(fit)

#-----------------------------------------
# Predicting using the fit model:
prediction <- predict(fit, newdata = house)

#-----------------------------------------
# Ploting the real vs predicted price:
plot(prediction, house$SalePrice, xlab = "Predicted Price", ylab = "Real Price",main = "Real vs Predicted Price")
abline(a=0, b=1)



#-----------------------------------------
# Q5:
# Finding the Rsqured:
RS <- summary(fit)$r.squared
print(RS)

#-----------------------------------------
#f statistic:
summary(fit)



```
Because `R Squared` is `0.77` and is larger than `0.5` we can suppose that it is not bad. Algtough we cannot say that `R Squared` use using a constant treshhold in all the problems and it depends on the nature of the problem, but in this case we can say th at the closer it is to 1 the better the `R Squared` is.  
For the `R Squared` we used the `summary` of our model.  
Regarding the `F-statistic`: the f statistic has a null hypothesis that all our coeficients are zero. Our model's f statistic is very high therefore its p-value is very low because it does not satistfy the assumption that we've made.  




## Question 6 and 7: 
We will look at the `p-values` of the ten variables that we chose. From these variables we will chose does that have a lesser `p-value`.  
We will again use the `summary` of our model to find these variables.Our new prediction is based on these new variables. The line indicates `y=x` which is our desired line.

```{r message = FALSE}
# HW6-6:
library(ggplot2)
library(gridExtra)
library(GGally)

#-----------------------------------------
# We will use does which have ** and *** in the p-value:
house = read.csv("/Users/shervin/Downloads/house/train.csv")
fit_new <- lm(SalePrice ~
            OverallQual +
            GrLivArea +
            GarageCars +
            TotalBsmtSF +
            X1stFlrSF +
            YearBuilt +
            YearRemodAdd, data = house)
summary(fit_new)
#-----------------------------------------
# Predicting using the new fit model:
prediction_new <- predict(fit_new, newdata = house)

#-----------------------------------------
# Ploting the real vs predicted price in the new model:
plot(prediction_new, house$SalePrice, xlab = " New Predicted Price", ylab = "Real Price",main = "Real vs New Predicted Price")
abline(a=0, b=1)

#-----------------------------------------
# Q7:

# Mean of resduals must be close to zero:
print(mean(fit_new$residuals))

#-----------------------------------------
#Normality:(top-right plot we see the fitted plot)
par(mfrow=c(2,2))
plot(fit_new)

#-----------------------------------------
# Constant Variance:
# From the top left and bottom left we see patterns that are 
# not flat, there it is no Constance Variance, therefore
# our assumption of constant variance does not apply here

#-----------------------------------------
# Independece or autocorrelation test:
par(mfrow=c(1,1))
acf(fit_new$residuals) # not corollated from the graph




```

We will check the assumptions using the four `plot`s and our new model.There are three essential assumptions:  
`Normality` says the `residuals` normally distributed.  
We will check this assumption the top-right plot. If these residuals behave normally they should be on the line. Which some of them are not. Therefore this assumption cannot be applied to this model.  
`Constant Variance` or `Homoscedasticity` says that the variances of the residuals must not change much with the change of our variable.  
As we can see from the top-left and bottom-left plots, this is not true. Because the fitted plots are not flat.  
`Independece` or `Autocorrelation test` says that the current value is independet from the previous values. We will check this assumption using 
`acf` function.  
If the values of others than the first index is not zero therefore it is correlated and our assumption is not right.  
But we see that from the graph that the other ones are indeed zero. Therefore this assumption is true.  

## Question 8 and 9:
From the `dplyr` package we use the `sample_n` function to sample 0.8 of the data as train data and the other data as test data. we will make a linear regression using the train data and test it on the test data.  
In the end we will use the `predict` function to find the standard error using `se.fit`. 
```{r message = FALSE }
# HW6- Q8-9:
library(ggplot2)
library(gridExtra)
library(GGally)
#-----------------------------------------
#reading data:
house = read.csv("/Users/shervin/Downloads/house/train.csv")

#-----------------------------------------
#train and test data:
train <- sample_frac(house, 0.8, replace =FALSE) 
temp<-as.numeric(rownames(train))
test<-house[-temp,]

#-----------------------------------------
# We will use does which have ** and *** in the p-value:
fit_new_train <- lm(SalePrice ~
                OverallQual +
                GrLivArea +
                GarageCars +
                TotalBsmtSF +
                X1stFlrSF +
                YearBuilt +
                YearRemodAdd, data = train)
summary(fit_new_train)

#-----------------------------------------
#Prediction:
prediction_new_test <- predict(fit_new_train, newdata = test,se.fit = TRUE)

#-----------------------------------------
#Standard Error:
print(sum(prediction_new_test$se.fit))

#-----------------------------------------
# Q9:
exp <- function(x,k){
  res_new = 0
    for(i in 1:k)
    {
      
      res_old = res_new
      res_new = cor(house$SalePrice,(x)^i)
      if(res_old > res_new)
      {
        break
      }
    }
  return(i-1)
}

#-----------------------------------------
# We will use correlation to see which exponential is the best
# for every variable:

i1 <- exp(house$OverallQual,100) 
print("OverallQual")
i1

i2 <- exp(house$GrLivArea,100)
print("GrLivArea")
i2

i3 <- exp(house$GarageCars,100)
print("GarageCars")
i3

i4 <- exp(house$TotalBsmtSF,100)
print("TotalBsmtSF")
i4

i5 <- exp(house$X1stFlrSF,100)
print("X1stFlrSF")
i5

i6 <- exp(house$YearBuilt,100)
print("YearBuilt")
i6

i7 <- exp(house$YearRemodAdd,100)
print("YearRemodAdd")
i7

#-----------------------------------------
# We will update our model:
fit_new_train_updated <- lm(SalePrice ~
                      OverallQual^3 +
                      GrLivArea +
                      GarageCars^2 +
                      TotalBsmtSF +
                      X1stFlrSF +
                      YearBuilt^78 +
                      YearRemodAdd^46, data = train)
summary(fit_new_train_updated)

#-----------------------------------------
prediction_new_test_updated <- predict(fit_new_train_updated, newdata = test,se.fit = TRUE)

```
For question 9 we will check the correlation of `SalePrice` with different powers of the variables and find the best suited variable.  
After that we will make these new powers as our new model and we'll use this for the next part.

## Question 10:
We will use the new model on the test data and see whether we'll get good results or not: 
```{r message = FALSE}
# HW6- Q8-10:
library(ggplot2)
library(gridExtra)
library(GGally)

#-----------------------------------------
#reading data:
house = read.csv("/Users/shervin/Downloads/house/train.csv")

#-----------------------------------------
#reading test data: 
test = read.csv("/Users/shervin/Downloads/house/test.csv")

#-----------------------------------------
#train data and our new final model:
train <- sample_frac(house, 0.8, replace =FALSE) 

#-----------------------------------------
# We will use does which have ** and *** in the p-value:
fit_new_train <- lm(SalePrice ~
                      OverallQual +
                      GrLivArea +
                      GarageCars +
                      TotalBsmtSF +
                      X1stFlrSF +
                      YearBuilt +
                      YearRemodAdd, data = train)
summary(fit_new_train)

#-----------------------------------------
#Prediction:
prediction_new_test <- predict(fit_new_train, newdata = test,se.fit = TRUE)


prediction_new_test = as.data.frame(prediction_new_test)

prediction_new_test %>% 
  mutate( Id = test$Id, SalePrice = fit) -> prediction_new_test

prediction_new_test %>% 
  select(Id,SalePrice) ->  prediction_new_test

prediction_new_test = as.matrix(prediction_new_test)

write.table(prediction_new_test, "shervinhakimi.txt", sep=",", row.names = FALSE)


```
After uploading the results in Kaggle my rank hase been: 4'681.  
this the link: 
`https://www.kaggle.com/c/house-prices-advanced-regression-techniques/leaderboard`

